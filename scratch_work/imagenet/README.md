# ImageNet experimentation

Peak performance with my cifar-10 experimentation was obtained with 4 layers so we'll start there.

This was a miserable experience. Wrestled with data wrangling for hours. Kicked off training only to see it was going to take hours.

Shrank the architecture of the network as I don't have the compute power to work with a dataset of this size in any reasonable amount of time.

Shrank the training to a max of 10 epochs. Desire is to finish this part of the project so that we can move on to the pre-trained, more powerful models.

Trained overnight. Woke up to .001% accuracy. 1/1000. Blargh.